#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Data loader designed for the "one patient, one graph" inductive model.

This module contains:
1. PatientGraphDataset: A custom PyTorch Geometric dataset class for loading
   individual graph data generated for each patient.
2. Demonstration of how to use torch_geometric.loader.DataLoader for efficient
   batching of these individual graphs.
"""

import os
import torch
import pandas as pd
from torch_geometric.data import Dataset, DataLoader
import os.path as osp
from tqdm import tqdm
import json
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset
from torch_geometric.data import Batch
import torch
import os.path as osp

class HierarchicalDataset(Dataset):
    """
    A custom PyTorch Dataset for loading hierarchical data generated by HierarchicalGraphBuilder.
    (Correct) Survival data is used for loss function, but not as input features.
    """
    def __init__(self, data_path, patient_ids, survival_df, processed_clinical_features=None):
        super().__init__()
        self.data_path = data_path
        self.patient_ids = patient_ids
        self.survival_df = survival_df
        # (核心修改) 存储处理后的临床特征
        self.processed_clinical_features = processed_clinical_features
        print(f"为 {len(self.patient_ids)} 个患者初始化数据集。")

    def __len__(self):
        return len(self.patient_ids)

    def __getitem__(self, idx):
        """
        Load and return a single patient's data package with survival data.
        (Correct) Survival data is used for loss function calculation, not as input features.
        """
        patient_id = self.patient_ids[idx]
        patient_file = osp.join(self.data_path, f"{patient_id}.pt")
        
        data_package = torch.load(patient_file, weights_only=False)

        # Update clinical features if processed ones are available
        if self.processed_clinical_features is not None and patient_id in self.processed_clinical_features:
            new_features = self.processed_clinical_features[patient_id]
            data_package['clinical_features'] = torch.tensor(new_features, dtype=torch.float).unsqueeze(0)

        # Attach survival data (for loss function, not input features)
        survival_info = self.survival_df.loc[patient_id]
        data_package['time'] = torch.tensor(survival_info['OS.time'] / 365.0, dtype=torch.float)
        data_package['event'] = torch.tensor(survival_info['OS'], dtype=torch.float)
        
        return data_package

def hierarchical_collate_fn(data_packages):
    """
    Custom collate function for manually batching packages containing hierarchical graph data.
    (Correct) Survival data is used for loss function calculation, not as input features.
    """
    # (Core fix) When batch size is very small, collate_fn may receive an empty list.
    # In this case, we should return an empty dict or None, allowing the training loop to safely skip it.
    if not data_packages:
        return None

    # -- (Debug logs, can be removed) --
    # print(f"\n[collate_fn] Received {len(data_packages)} data packages...")
    # for i, pkg in enumerate(data_packages):
    #     if 'intra_pathway_graphs' not in pkg or not pkg['intra_pathway_graphs']:
    #          print(f"  - Patient {i} has no graph data.")
    #     else:
    #         print(f"  - Patient {i} has {len(pkg['intra_pathway_graphs'])} graphs.")
    # -- End debug --

    list_of_graph_lists = [pkg['intra_pathway_graphs'] for pkg in data_packages if pkg.get('intra_pathway_graphs')]
    
    # If after filtering, no patients have graph data, this is an invalid batch
    if not list_of_graph_lists:
        return None

    clinical_features_list = [pkg['clinical_features'] for pkg in data_packages if pkg.get('intra_pathway_graphs')]
    time_list = [pkg['time'] for pkg in data_packages if pkg.get('intra_pathway_graphs')]
    event_list = [pkg['event'] for pkg in data_packages if pkg.get('intra_pathway_graphs')]
    
    # (Fix) If filtered lists are empty (unlikely since we checked above), confirm again
    if not clinical_features_list:
        return None

    all_pathway_graphs = [graph for sublist in list_of_graph_lists for graph in sublist]
    
    pathway_to_patient_map = []
    patient_idx_counter = 0
    for sublist in list_of_graph_lists:
        pathway_to_patient_map.extend([patient_idx_counter] * len(sublist))
        patient_idx_counter += 1

    graphs_batch = Batch.from_data_list(all_pathway_graphs)
    graphs_batch.pathway_to_patient_batch_map = torch.tensor(pathway_to_patient_map, dtype=torch.long)
    
    # (Added) Collect and pass the globally unique index for each pathway
    pathway_indices = [g.pathway_idx.item() for g in all_pathway_graphs]
    graphs_batch.pathway_idx = torch.tensor(pathway_indices, dtype=torch.long)

    clinical_features = torch.cat(clinical_features_list, dim=0)
    time = torch.stack(time_list)
    event = torch.stack(event_list)
    
    # print(f"[collate_fn] Batch processing completed. Graphs: {len(all_pathway_graphs)}, Patients: {len(time)}")
    return {
        'graphs_batch': graphs_batch,
        'clinical_features': clinical_features,
        'time': time,
        'event': event
    }


class PatientGraphDataset(Dataset):
    """
    Custom PyTorch Dataset for the "one patient, one graph" approach.
    Modified version: Loads graph data and attaches normalized clinical features.
    """
    def __init__(self, data_path, all_patient_ids, train_ids=None):
        """
        Initialize the dataset.

        Parameters:
            data_path (str): Path to the 'processed' directory.
            all_patient_ids (list): List of patient IDs to load.
            train_ids (list, optional): Patient IDs for training set, only used to fit the scaler.
                                   If None, fits on all data.
        """
        self.data_path = data_path
        self.patient_graphs_path = osp.join(data_path, 'patient_graphs')
        self.patient_ids = all_patient_ids
        
        # --- Data Loading and Preprocessing ---
        # 1. Load and normalize clinical features
        clinical_features_path = osp.join(self.data_path, 'patient_clinical_features.csv')
        clinical_df = pd.read_csv(clinical_features_path, index_col=0)
        clinical_df.index = clinical_df.index.astype(str)
        
        scaler = StandardScaler()
        # If train IDs are provided, fit scaler only on training data
        fit_ids = train_ids if train_ids is not None else self.patient_ids
        
        # Ensure fit IDs exist in DataFrame
        fit_ids = [pid for pid in fit_ids if pid in clinical_df.index]
        if not fit_ids:
            raise ValueError("'train_ids' for fitting the scaler did not find any matches in the clinical features file.")
            
        scaler.fit(clinical_df.loc[fit_ids])
        
        # Transform all data
        normalized_values = scaler.transform(clinical_df)
        self.normalized_clinical_df = pd.DataFrame(normalized_values, index=clinical_df.index, columns=clinical_df.columns)
        
        # Fix: Change from preloading to lazy loading, remove preloaded_data
        # self.preloaded_data = self._preload_data()
        
        # Fix: Filter valid patient IDs with clinical data during initialization
        self.patient_ids = [pid for pid in self.patient_ids if pid in self.normalized_clinical_df.index]
        print(f"Dataset initialized. Will provide data for {len(self.patient_ids)} patients.")


    def __len__(self):
        # Length is now directly the number of valid patient IDs
        return len(self.patient_ids)

    def __getitem__(self, idx):
        """
        Get a sample (lazy loading version).
        Dynamically loads graph data and attaches features on each call.
        """
        patient_id = self.patient_ids[idx]
        graph_path = osp.join(self.patient_graphs_path, f"{patient_id}.pt")

        if not osp.exists(graph_path):
            raise FileNotFoundError(f"图文件未找到: {graph_path}")
            
        # 加载单个图数据
        # 修正: 显式设置 weights_only=False 以允许加载复杂的 HeteroData 对象
        data = torch.load(graph_path, map_location='cpu', weights_only=False)

        # 确保 patient_id 存在且为字符串
        if hasattr(data, 'patient_id'):
            data.patient_id = str(data.patient_id)
        else:
            data.patient_id = patient_id

        # 附加归一化后的临床特征
        if patient_id in self.normalized_clinical_df.index:
            clinical_features = self.normalized_clinical_df.loc[patient_id].values
            data.clinical_features = torch.tensor(clinical_features, dtype=torch.float).unsqueeze(0)
        else:
            # 作为备用，如果找不到临床数据，则填充零向量
            # 注意：这需要模型能够处理这种情况
            num_features = len(self.normalized_clinical_df.columns)
            data.clinical_features = torch.zeros(1, num_features, dtype=torch.float)

        return data

def main():
    """演示如何使用修改后的Dataset和DataLoader"""
    current_dir = osp.dirname(osp.abspath(__file__))
    project_root = osp.dirname(current_dir)
    data_path = osp.join(project_root, "data", "processed")
    
    print(f"从以下路径加载数据: {data_path}")

    # --- 演示工作流程 ---
    # 1. 首先，获取所有患者ID并进行划分 (这通常在训练脚本中完成)
    mappings_path = osp.join(data_path, 'id_mappings.json')
    with open(mappings_path, 'r') as f:
        id_mappings = json.load(f)
    all_patient_ids = list(id_mappings['patient_id_to_idx'].keys())
    
    # 模拟训练/验证/测试集划分
    from sklearn.model_selection import train_test_split
    train_val_ids, test_ids = train_test_split(all_patient_ids, test_size=0.2, random_state=42)
    train_ids, val_ids = train_test_split(train_val_ids, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2

    # 2. 创建数据集实例
    # 创建训练集时，传入 train_ids 以正确拟合归一化器
    train_dataset = PatientGraphDataset(data_path=data_path, all_patient_ids=train_ids, train_ids=train_ids)
    # 创建验证/测试集时，也传入 train_ids 以使用相同的归一化器
    val_dataset = PatientGraphDataset(data_path=data_path, all_patient_ids=val_ids, train_ids=train_ids)
    test_dataset = PatientGraphDataset(data_path=data_path, all_patient_ids=test_ids, train_ids=train_ids)

    print(f"\n成功创建数据集 - 训练集: {len(train_dataset)}, 验证集: {len(val_dataset)}, 测试集: {len(test_dataset)}")
    
    # 3. 从训练数据集中获取一个样本进行检查
    if len(train_dataset) > 0:
        sample_graph = train_dataset[0]
        print("\n--- 单个样本检查 ---")
        print(sample_graph)
        print(f"患者ID: {sample_graph.patient_id}")
        
        if hasattr(sample_graph, 'clinical_features'):
            print(f"临床特征 (clinical_features) 形状: {sample_graph.clinical_features.shape}")
            print(f"临床特征 (前5个值): {sample_graph.clinical_features[0, :5].numpy()}")
        else:
            print("警告: 样本中未找到 'clinical_features' 属性。")
        print("---------------------\n")
        
    # 4. 创建DataLoader
    data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    
    # 5. 从DataLoader中获取一个批次进行检查
    if len(data_loader) > 0:
        sample_batch = next(iter(data_loader))
        print("--- 单个批次检查 ---")
        print(sample_batch)
        print(f"批次中的图数量: {sample_batch.num_graphs}")
            
        if hasattr(sample_batch, 'clinical_features'):
            print(f"批次临床特征 (clinical_features) 形状: {sample_batch.clinical_features.shape}")
        else:
            print("警告: 批次中未找到 'clinical_features' 属性。")
        print("---------------------\n")

if __name__ == '__main__':
    main()
